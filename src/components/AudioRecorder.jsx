import React, { useState, useRef } from 'react';
import Card from './ui/Card';
import Button from './ui/Button';
import { Title, Text } from './ui/Typography';
import { AudioOutlined, StopOutlined } from '@ant-design/icons';
import Recorder from 'recorder-js';

const AudioRecorder = ({ onAudioData }) => {
  const [isRecording, setIsRecording] = useState(false);
  const [audioLevel, setAudioLevel] = useState(0); // ç”¨äºå¯è§†åŒ–

  const recorderRef = useRef(null);
  const audioContextRef = useRef(null);
  const analyserRef = useRef(null);
  const animationRef = useRef(null);
  const sourceRef = useRef(null);
  const streamRef = useRef(null);
  const canvasRef = useRef(null);

  // éŸ³é¢‘é™éŸ³è£å‰ªå‡½æ•°
  const trimSilence = async (audioBlob) => {
    try {
      // å°† Blob è½¬æ¢ä¸º ArrayBuffer
      const arrayBuffer = await audioBlob.arrayBuffer();
      const audioContext = new (window.AudioContext || window.webkitAudioContext)();
      
      // è§£ç éŸ³é¢‘æ•°æ®
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      const channelData = audioBuffer.getChannelData(0); // è·å–ç¬¬ä¸€ä¸ªå£°é“
      
      // è®¡ç®—éŸ³é¢‘èƒ½é‡é˜ˆå€¼ï¼ˆå¯è°ƒæ•´ï¼‰
      const threshold = 0.01; // é™éŸ³é˜ˆå€¼
      const minSilenceLength = 0.1; // æœ€å°é™éŸ³é•¿åº¦ï¼ˆç§’ï¼‰
      const sampleRate = audioBuffer.sampleRate;
      const minSilenceSamples = Math.floor(minSilenceLength * sampleRate);
      
      // æ‰¾åˆ°ç¬¬ä¸€ä¸ªéé™éŸ³ä½ç½®
      let startIndex = 0;
      for (let i = 0; i < channelData.length; i++) {
        if (Math.abs(channelData[i]) > threshold) {
          startIndex = i;
          break;
        }
      }
      
      // æ‰¾åˆ°æœ€åä¸€ä¸ªéé™éŸ³ä½ç½®
      let endIndex = channelData.length - 1;
      for (let i = channelData.length - 1; i >= 0; i--) {
        if (Math.abs(channelData[i]) > threshold) {
          endIndex = i;
          break;
        }
      }
      
      // ç¡®ä¿æœ‰è¶³å¤Ÿçš„éŸ³é¢‘æ•°æ®
      if (endIndex <= startIndex) {
        console.log('éŸ³é¢‘å¤ªçŸ­ï¼Œæ— æ³•è£å‰ª');
        return audioBlob;
      }
      
      // æ·»åŠ ä¸€äº›ç¼“å†²ï¼ˆä¿ç•™å‰åå„0.1ç§’ï¼‰
      const bufferSamples = Math.floor(0.1 * sampleRate);
      startIndex = Math.max(0, startIndex - bufferSamples);
      endIndex = Math.min(channelData.length - 1, endIndex + bufferSamples);
      
      // åˆ›å»ºæ–°çš„éŸ³é¢‘ç¼“å†²åŒº
      const trimmedLength = endIndex - startIndex + 1;
      const trimmedBuffer = audioContext.createBuffer(1, trimmedLength, sampleRate);
      const trimmedChannelData = trimmedBuffer.getChannelData(0);
      
      // å¤åˆ¶è£å‰ªåçš„éŸ³é¢‘æ•°æ®
      for (let i = 0; i < trimmedLength; i++) {
        trimmedChannelData[i] = channelData[startIndex + i];
      }
      
      // å°† AudioBuffer è½¬æ¢å› Blob
      const trimmedBlob = await audioBufferToBlob(trimmedBuffer);
      
      console.log(`éŸ³é¢‘è£å‰ª: åŸå§‹é•¿åº¦ ${channelData.length} æ ·æœ¬ï¼Œè£å‰ªå ${trimmedLength} æ ·æœ¬`);
      return trimmedBlob;
      
    } catch (error) {
      console.error('éŸ³é¢‘è£å‰ªå¤±è´¥:', error);
      return audioBlob; // å¦‚æœè£å‰ªå¤±è´¥ï¼Œè¿”å›åŸå§‹éŸ³é¢‘
    }
  };
  
  // AudioBuffer è½¬ Blob çš„è¾…åŠ©å‡½æ•°
  const audioBufferToBlob = async (audioBuffer) => {
    const sampleRate = audioBuffer.sampleRate;
    const numberOfChannels = audioBuffer.numberOfChannels;
    const length = audioBuffer.length;
    
    // åˆ›å»º WAV æ ¼å¼çš„ ArrayBuffer
    const buffer = new ArrayBuffer(44 + length * numberOfChannels * 2);
    const view = new DataView(buffer);
    
    // WAV æ–‡ä»¶å¤´
    const writeString = (offset, string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i));
      }
    };
    
    writeString(0, 'RIFF');
    view.setUint32(4, 36 + length * numberOfChannels * 2, true);
    writeString(8, 'WAVE');
    writeString(12, 'fmt ');
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true);
    view.setUint16(22, numberOfChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * numberOfChannels * 2, true);
    view.setUint16(32, numberOfChannels * 2, true);
    view.setUint16(34, 16, true);
    writeString(36, 'data');
    view.setUint32(40, length * numberOfChannels * 2, true);
    
    // å†™å…¥éŸ³é¢‘æ•°æ®
    let offset = 44;
    for (let i = 0; i < length; i++) {
      for (let channel = 0; channel < numberOfChannels; channel++) {
        const sample = Math.max(-1, Math.min(1, audioBuffer.getChannelData(channel)[i]));
        view.setInt16(offset, sample * 0x7FFF, true);
        offset += 2;
      }
    }
    
    return new Blob([buffer], { type: 'audio/wav' });
  };

  // å¯è§†åŒ–æ¸²æŸ“
  const drawVisualizer = () => {
    if (!analyserRef.current || !canvasRef.current) return;
    const canvas = canvasRef.current;
    const ctx = canvas.getContext('2d');
    const bufferLength = analyserRef.current.fftSize;
    const dataArray = new Uint8Array(bufferLength);
    analyserRef.current.getByteTimeDomainData(dataArray);
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.lineWidth = 2;
    ctx.strokeStyle = '#3b82f6';
    ctx.beginPath();
    const sliceWidth = canvas.width / bufferLength;
    let x = 0;
    for (let i = 0; i < bufferLength; i++) {
      const v = dataArray[i] / 128.0;
      const y = (v * canvas.height) / 2;
      if (i === 0) {
        ctx.moveTo(x, y);
      } else {
        ctx.lineTo(x, y);
      }
      x += sliceWidth;
    }
    ctx.lineTo(canvas.width, canvas.height / 2);
    ctx.stroke();
    // è®¡ç®—éŸ³é‡ç­‰çº§ç”¨äºåç»­æ‰©å±•
    const rms = Math.sqrt(dataArray.reduce((sum, v) => sum + (v - 128) * (v - 128), 0) / bufferLength);
    setAudioLevel(rms);
    animationRef.current = requestAnimationFrame(drawVisualizer);
  };

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;
      const AudioContext = window.AudioContext || window.webkitAudioContext;
      const audioContext = new AudioContext({ sampleRate: 16000 });
      audioContextRef.current = audioContext;
      const recorder = new Recorder(audioContext, {
        numChannels: 1,
        sampleRate: 16000
      });
      recorderRef.current = recorder;
      await recorder.init(stream);
      recorder.start();
      // å¯è§†åŒ–
      const source = audioContext.createMediaStreamSource(stream);
      sourceRef.current = source;
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 256;
      analyserRef.current = analyser;
      source.connect(analyser);
      animationRef.current = requestAnimationFrame(drawVisualizer);
      setIsRecording(true);
    } catch (error) {
      // å¯ä»¥ç”¨å…¨å±€Toastæç¤º
    }
  };

  const stopRecording = async () => {
    if (recorderRef.current && isRecording) {
      const { blob } = await recorderRef.current.stop();
      setIsRecording(false);
      
      // å¯¹éŸ³é¢‘è¿›è¡Œé™éŸ³è£å‰ª
      const trimmedBlob = await trimSilence(blob);
      
      // ç›´æ¥å‘é€éŸ³é¢‘ï¼Œä¸ä¿å­˜åˆ°æœ¬åœ°çŠ¶æ€
      if (onAudioData) {
        onAudioData(trimmedBlob);
      }
      
      // æ¸…ç†èµ„æº
      if (audioContextRef.current) {
        audioContextRef.current.close();
        audioContextRef.current = null;
      }
      if (animationRef.current) {
        cancelAnimationFrame(animationRef.current);
        animationRef.current = null;
      }
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
        streamRef.current = null;
      }
      
      // é‡ç½®çŠ¶æ€ï¼Œä¸ä¿å­˜éŸ³é¢‘URLå’ŒBlob
    }
  };

  // ç»„ä»¶å¸è½½æ—¶æ¸…ç†åŠ¨ç”»å’ŒéŸ³é¢‘æµ
  React.useEffect(() => {
    return () => {
      if (animationRef.current) cancelAnimationFrame(animationRef.current);
      if (streamRef.current) streamRef.current.getTracks().forEach(track => track.stop());
    };
  }, []);

  return (
    <Card style={{ padding: 16, borderRadius: 12, boxShadow: '0 2px 12px rgba(0,0,0,0.08)' }}>
      <div style={{ textAlign: 'center', marginBottom: 16 }}>
        <Title level={5} style={{ marginBottom: 4, fontSize: 16 }}>éŸ³é¢‘å½•åˆ¶</Title>
      </div>
      
      {/* éŸ³é¢‘å¯è§†åŒ– */}
      <div style={{ marginBottom: 16, textAlign: 'center' }}>
        <canvas 
          ref={canvasRef} 
          width={300} 
          height={40} 
          style={{ 
            border: '1px solid #e2e8f0', 
            borderRadius: 6, 
            background: '#f8fafc' 
          }} 
        />
      </div>
      
      {/* æ§åˆ¶æŒ‰é’® */}
      <div style={{ display: 'flex', gap: 8, justifyContent: 'center', flexWrap: 'wrap' }}>
        {!isRecording && (
          <Button 
            type="primary" 
            icon={<AudioOutlined />} 
            onClick={startRecording}
            style={{ minWidth: 140, height: 40, fontSize: 14 }}
          >
            ç‚¹å‡»è¯´è¯
          </Button>
        )}
        
        {isRecording && (
          <Button 
            danger 
            icon={<StopOutlined />} 
            onClick={stopRecording}
            style={{ minWidth: 140, height: 40, fontSize: 14 }}
          >
            ç»“æŸè¯´è¯
          </Button>
        )}
      </div>
      
      {/* çŠ¶æ€æç¤º */}
      {isRecording && (
        <div style={{ 
          marginTop: 12, 
          textAlign: 'center', 
          color: '#ef4444', 
          fontSize: 12,
          fontWeight: 500 
        }}>
          ğŸ¤ æ­£åœ¨å½•åˆ¶ä¸­...
        </div>
      )}
    </Card>
  );
};

export default AudioRecorder; 